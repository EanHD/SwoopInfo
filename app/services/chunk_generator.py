from models.chunk import ServiceChunk, ChunkType, SourceCitation
from models.vehicle import Vehicle, VehicleConcern
from services.openrouter import openrouter
from services.supabase_client import supabase_service
from services.nhtsa import nhtsa_service
from services.carquery import carquery_service
from services.vehicledatabases import vehicledatabases_service
# OPTIMIZED: Use smart_search instead of individual Brave/Tavily services
from services.smart_search import smart_search
from services.ddg_client import ddg_service
from services.advanced_generator import advanced_generator
from services.performance import (
    prompt_cache,
    llm_semaphore,
    build_vehicle_context,
    parallel_generate_with_semaphore,
)
import asyncio
import json
import base64
import re
from typing import Optional, Dict, Any, List


# FACTORY MANUAL CSS - Professional service document styling
FACTORY_MANUAL_CSS = """
<style>
.service-doc {
  background: #f8f8f8;
  font-family: "Helvetica Neue", Helvetica, Arial, sans-serif;
  line-height: 1.6;
  color: #1a1a1a;
  padding: 30px;
  max-width: 900px;
  margin: 0 auto;
  border: 1px solid #ddd;
  box-shadow: 0 4px 20px rgba(0,0,0,0.08);
}
.header-stripe {
  height: 12px;
  background: linear-gradient(90deg, #003087, #005eb8, #0072ce);
  margin-bottom: 20px;
}
h1, h2 {
  color: #003087;
  border-bottom: 3px solid #003087;
  padding-bottom: 6px;
  margin-top: 40px;
  font-weight: 600;
  text-transform: uppercase;
  letter-spacing: 1px;
}
h3 {
  color: #005eb8;
  margin-top: 30px;
  font-weight: 600;
}
.step {
  background: white;
  padding: 18px 22px;
  margin: 18px 0;
  border-left: 6px solid #005eb8;
  box-shadow: 0 2px 8px rgba(0,0,0,0.06);
  border-radius: 0 6px 6px 0;
}
.parts-box, .tools-box {
  background: #f0f7ff;
  border: 2px solid #005eb8;
  padding: 16px;
  margin: 20px 0;
  border-radius: 8px;
  font-weight: 500;
}
.warning {
  background: #fff0f0;
  border-left: 8px solid #c00;
  padding: 16px;
  margin: 20px 0;
  border-radius: 0 6px 6px 0;
  font-weight: bold;
  color: #900;
}
.note {
  background: #f0f7ff;
  border: 1px solid #005eb8;
  padding: 12px 16px;
  margin: 16px 0;
  border-radius: 6px;
  color: #003087;
}
ul { padding-left: 20px; }
li { margin: 8px 0; }
</style>
"""


def wrap_in_factory_manual_html(
    content: str, title: str = "", vehicle_info: str = ""
) -> str:
    """Wrap HTML content in factory manual styling."""
    return f"""
{FACTORY_MANUAL_CSS}
<div class="service-doc">
  <div class="header-stripe"></div>
  {f'<h1>{title.upper()}</h1>' if title else ''}
  {f'<h2>{vehicle_info}</h2>' if vehicle_info else ''}
  {content}
</div>
"""


class ChunkGenerator:

    def _get_content_id_for_title(self, title: str) -> str:
        """
        Get the correct content_id for a given title.
        Strictly snake_case(title). No fuzzy matching.
        """
        return (
            re.sub(r"[^a-z0-9_]", "_", title.lower())
            .replace(" ", "_")
            .replace("&", "and")
            .replace("-", "_")
            .replace("__", "_")
            .strip("_")
        )

    async def fetch_real_data(
        self,
        vehicle: Vehicle,
        chunk_type: ChunkType,
        concern: str,
        dtc_codes: List[str] = [],
    ) -> Dict[str, Any]:
        """
        OPTIMIZED: Fetch real data using smart tiered search.
        
        Strategy:
        1. FREE sources first (NHTSA, CarQuery) - always
        2. Smart search (Brave + conditional Tavily) - cost optimized
        3. Aggressive caching (24hr for search results)
        4. Consensus-based confidence scoring
        
        OLD: 12 Brave queries + 1 Tavily = ~$0.02/generation
        NEW: 1-2 Brave + conditional Tavily = ~$0.003-0.007/generation
        ~70-85% cost reduction without quality loss
        """
        # PERF: Check cache first for this vehicle + chunk_type combo
        cache_key = f"api_data:{vehicle.key}:{chunk_type}:{concern[:50]}"
        cached = await prompt_cache.get(cache_key)
        if cached:
            return cached

        combined_data = {
            "facts": [],
            "citations": [],
            "api_cost": 0.0,
            "sources_found": 0,
            "consensus": {},  # NEW: Consensus tracking
        }

        # Collect all API tasks for maximum parallelism
        api_tasks = []
        task_labels = []

        # Priority 1: NHTSA (always call - it's FREE)
        if chunk_type in ["known_issues", "diag_flow", "torque_spec", "fluid_capacity", 
                          "known_issue", "diagnostic_info"]:
            api_tasks.append(nhtsa_service.get_tsbs_and_recalls(vehicle))
            task_labels.append("nhtsa")

        # Priority 2: CarQuery (FREE vehicle database)
        if chunk_type in ["fluid_capacity", "torque_spec", "removal_steps", 
                          "brake_spec", "tire_spec"]:
            api_tasks.append(carquery_service.get_trims(vehicle))
            task_labels.append("carquery")

        # Priority 3: SMART SEARCH (replaces old Brave 12-query + Tavily pattern)
        # Uses optimized 1-2 query strategy with consensus scoring
        api_tasks.append(
            smart_search.search_for_chunk(vehicle, chunk_type, concern)
        )
        task_labels.append("smart_search")

        # DDGS for instant answers (still free, lightweight)
        if ddg_service.enabled and chunk_type in ["fluid_capacity", "torque_spec"]:
            api_tasks.append(ddg_service.search_instant_answers(vehicle, concern))
            task_labels.append("ddgs")

        # PERF: Run ALL API calls in parallel
        if api_tasks:
            results = await asyncio.gather(*api_tasks, return_exceptions=True)

            for i, res in enumerate(results):
                label = task_labels[i]

                if isinstance(res, Exception):
                    print(f"‚ö†Ô∏è {label} failed: {res}")
                    continue

                if label == "nhtsa":
                    if res.get("citations"):
                        combined_data["citations"].extend(res["citations"])
                        combined_data["sources_found"] += 1
                    if res.get("recalls"):
                        combined_data["facts"].append(
                            f"NHTSA Recall Data: {len(res['recalls'])} recalls found"
                        )

                elif label == "carquery":
                    if res.get("success") and res.get("matching_trim"):
                        trim = res["matching_trim"]
                        combined_data["facts"].append(
                            f"Engine: {trim.get('model_engine_type', 'N/A')}, {trim.get('model_engine_cc', 'N/A')}cc"
                        )
                        combined_data["citations"].append(
                            SourceCitation(
                                source_type="api",
                                url="https://www.carqueryapi.com",
                                description="CarQuery Vehicle Database",
                                confidence=0.85,
                            )
                        )
                        combined_data["sources_found"] += 1

                elif label == "smart_search":
                    # NEW: Handle optimized smart search results
                    if not res.get("cached"):
                        combined_data["api_cost"] += res.get("cost", 0.0)
                    
                    combined_data["citations"].extend(res.get("citations", []))
                    combined_data["sources_found"] += res.get("sources_found", 0)
                    combined_data["facts"].extend(res.get("facts", []))
                    combined_data["consensus"] = res.get("consensus", {})
                    
                    # Log cost savings
                    if res.get("cached"):
                        print(f"   ‚ö° Smart search: CACHED (saved ~$0.003)")
                    else:
                        print(f"   üí∞ Smart search cost: ${res.get('cost', 0):.4f}")

                elif label == "ddgs":
                    if res.get("success"):
                        combined_data["citations"].extend(res.get("citations", []))
                        combined_data["sources_found"] += len(res.get("results", []))

                        for result in res.get("results", []):
                            title = result.get("title", "")
                            body = result.get("body", "")[:200]
                            combined_data["facts"].append(f"[DDGS] {title}: {body}")

        # PERF: Cache the result for 5 minutes
        await prompt_cache.set(cache_key, combined_data)

        return combined_data

    async def identify_needed_chunks(
        self, concern: VehicleConcern
    ) -> list[tuple[ChunkType, str]]:
        """
        Use Grok-4-Fast to determine which chunk types are needed for this concern.
        PERFORMANCE: Uses cache + semaphore for deduplication and rate limiting.
        Returns: list of (chunk_type, context/title) tuples
        """
        # PERF: Cache key based on vehicle + concern (normalized)
        cache_key = (
            f"needed_chunks:{concern.vehicle.key}:{concern.concern.lower()[:100]}"
        )
        cached = await prompt_cache.get(cache_key)
        if cached:
            print(f"‚ö° Cache hit for identify_needed_chunks")
            return cached

        prompt = f"""You are an expert automotive diagnostic assistant.

Vehicle: {concern.vehicle.year} {concern.vehicle.make} {concern.vehicle.model} {concern.vehicle.engine}
Customer Concern: {concern.concern}
DTC Codes: {concern.dtc_codes or "None"}

Determine which specific information chunks would be most helpful for a mobile mechanic diagnosing this issue.

Available chunk types:
- fluid_capacity: Oil, coolant, brake fluid specs
- torque_spec: Fastener torque values
- part_location: Where components are located
- known_issues: Common failures, TSBs, recalls
- removal_steps: Step-by-step R&R procedures
- wiring_diagram: Electrical diagrams
- diag_flow: Diagnostic decision trees

Return ONLY a JSON array of objects with "chunk_type" and "title" fields.
Example: [{{"chunk_type": "known_issues", "title": "Common No-Start Issues"}}, {{"chunk_type": "part_location", "title": "Fuel Pump Driver Module Location"}}]

Focus on the MOST RELEVANT chunks (3-6 total). Be specific in titles."""

        # PERF: Use semaphore to limit concurrent LLM calls
        async with llm_semaphore:
            response, cost = await openrouter.chat_completion(
                "ingestion", [{"role": "user", "content": prompt}], temperature=0.3
            )

        try:
            # Clean up markdown code blocks if present
            response_text = response.strip()
            if response_text.startswith("```json"):
                response_text = response_text[7:]
            if response_text.startswith("```"):
                response_text = response_text[3:]
            if response_text.endswith("```"):
                response_text = response_text[:-3]

            chunks_needed = json.loads(response_text.strip())
            result = [(item["chunk_type"], item["title"]) for item in chunks_needed]

            # PERF: Cache the result
            await prompt_cache.set(cache_key, result)

            return result
        except Exception as e:
            print(f"Error parsing needed chunks: {e}")
            print(f"Raw response: {response}")
            return []

    async def generate_chunk(
        self,
        vehicle: Vehicle,
        chunk_type: ChunkType,
        title: str,
        context: str,
        dtc_codes: List[str] = [],
        template_version: str = "1.0",
    ) -> tuple[ServiceChunk, float]:
        """
        Generate a single chunk using ENHANCED CONSENSUS pipeline:
        1. Fetch REAL data from ALL sources (NHTSA, CarQuery, Brave, Tavily, DDGS)
        2. Apply source quality weighting (OEM manuals > tech sites > forums > blogs)
        3. Cross-validate: multi-source agreement = verified, single-source = flag for review
        4. Sanity check: AI validates data makes logical sense
        5. Format output with consensus badges and confidence scores
        """
        total_cost = 0.0
        source_citations = []

        # NEW: Fetch real data from APIs FIRST
        real_data = await self.fetch_real_data(vehicle, chunk_type, context, dtc_codes)
        total_cost += real_data.get("api_cost", 0.0)
        source_citations.extend(real_data.get("citations", []))

        # Build research context from real API data
        api_facts = "\n".join(real_data.get("facts", []))
        sources_found = real_data.get("sources_found", 0)

        # Step 1: Generate structured data based on chunk_type

        # DIAGRAMS DISABLED UNTIL PERFECT - Return placeholder chunk instead
        if chunk_type in ["diagram", "wiring_diagram", "diagram_svg", "belt_routing"]:
            print(f"‚è∏Ô∏è DIAGRAMS DISABLED: Returning placeholder for {title}")
            chunk = ServiceChunk(
                vehicle_key=vehicle.key,
                chunk_type=chunk_type,
                title=title,
                content_html="<p>Diagram coming soon. This feature is being perfected and will be available in the next update.</p>",
                content_text="Diagram coming soon. This feature is being perfected and will be available in the next update.",
                data={
                    "placeholder": True,
                    "message": "Belt routing diagram will be added in the next update.",
                },
                tags=[chunk_type, vehicle.make.lower(), vehicle.model.lower()],
                source_citations=source_citations,
                verification_status="pending_review",
                requires_human_review=False,
                verified=False,
                consensus_score=0.0,
                consensus_badge="Coming Soon",
                cost_to_generate=0.0,
                template_version=template_version,
            )
            return chunk, 0.0

        # DISABLED UNTIL DIAGRAMS ARE PERFECT - Original diagram generation code below
        if False:  # DIAGRAMS DISABLED UNTIL PERFECT
            if chunk_type in ["diagram", "wiring_diagram"]:
                print(
                    f"üîå Delegating diagram generation for {title} to AdvancedGenerator..."
                )

                # Extract system/component from title/context if possible, or use title as component
                system = "Electrical"  # Default
                component = title

                diagram_result = await advanced_generator.generate_wiring_diagram(
                    vehicle_key=vehicle.key,
                    year=vehicle.year,
                    make=vehicle.make,
                    model=vehicle.model,
                    system=system,
                    component=component,
                )

                chunk_data = diagram_result.get("data", {})

                # Force image_url generation using mermaid.ink
                mermaid_code = chunk_data.get("diagram_code")
                if mermaid_code:
                    encoded_code = base64.urlsafe_b64encode(
                        mermaid_code.encode("utf-8")
                    ).decode("utf-8")
                    chunk_data["image_url"] = f"https://mermaid.ink/img/{encoded_code}"
                    chunk_data["diagram_code"] = mermaid_code

                text_content = chunk_data.get(
                    "notes", "Diagram generated via Vision Pipeline"
                )
                html_content = ""  # Diagrams use image_url/mermaid_code, not HTML body

                # Add sources from diagram result
                for src in diagram_result.get("sources", []):
                    source_citations.append(
                        SourceCitation(
                            source_type="vision_analysis",
                            url="",
                            description=src,
                            confidence=diagram_result.get("source_confidence", 0.5),
                        )
                    )

                # Create ServiceChunk immediately and return
                # Force chunk_type to 'wiring_diagram' for DB compatibility if 'diagram' was requested

                if chunk_data.get("diagram_type") == "svg":
                    db_chunk_type = "diagram_svg"
                    # Store raw SVG in content_text as requested
                    text_content = chunk_data.get("diagram_code", "")
                else:
                    db_chunk_type = (
                        "wiring_diagram" if chunk_type == "diagram" else chunk_type
                    )

                chunk = ServiceChunk(
                    vehicle_key=vehicle.key,
                    chunk_type=db_chunk_type,
                    title=title,
                    content_html=html_content,
                    content_text=text_content,
                    data=chunk_data,
                    tags=[
                        chunk_type,
                        vehicle.make.lower(),
                        vehicle.model.lower(),
                        "diagram",
                    ],
                    source_citations=source_citations,
                    verification_status="pending_review",  # Diagrams always need review
                    requires_human_review=True,
                    verified=False,
                    consensus_score=diagram_result.get("source_confidence", 0.5),
                    consensus_badge="Vision Generated",
                    cost_to_generate=total_cost,
                    template_version=template_version,
                )
                return chunk, total_cost

        # SPEC chunks need JSON, others need HTML
        is_spec_chunk = chunk_type in [
            "fluid_capacity",
            "torque_spec",
            "labor_time",
            "part_info",
        ]

        if is_spec_chunk:
            # SPEC CHUNKS: Return pure JSON array
            if chunk_type == "torque_spec":
                research_prompt = f"""You are an expert automotive technician extracting torque specifications.

REAL DATA FROM SOURCES:
{api_facts if api_facts else "No specific data found - use general knowledge for this vehicle"}

Vehicle: {vehicle.year} {vehicle.make} {vehicle.model} {vehicle.engine}
Specification Type: {chunk_type}
Specific Item: {title}

CRITICAL ENGINE-SPECIFIC REQUIREMENTS:
- Generate content SPECIFIC to the {vehicle.engine} engine ONLY
- Do NOT mix data from other engines in the {vehicle.model} lineup
- For {vehicle.year} {vehicle.make} {vehicle.model} {vehicle.engine}:
  * Verify all torque specs match THIS ENGINE configuration
  * If data varies by engine option, clearly state which engine this applies to

TORQUE SPECIFICATION REQUIREMENTS:
- If Torque-To-Yield (TTY) bolts are used, add a spec item: {{"label": "Bolt Type", "value": "TTY - Replace with NEW bolts", "unit": ""}}
- If tightening sequence matters, include: {{"label": "Tightening Pattern", "value": "Center outward in circular pattern", "unit": ""}}
- For multi-step torque procedures, create separate entries for each step (e.g., Step 1, Step 2)

CRITICAL INSTRUCTIONS:
- Focus ONLY on the torque specification for the requested item.
- Do NOT include oil filter part numbers, oil grades, or fluid capacities unless explicitly asked for.
- If the item is "drain plug", just provide the torque.
- Focus solely on the requested spec. Do not mention unrelated parts like filters, grades, or capacities unless directly asked.

REQUIRED OUTPUT FORMAT (JSON array only, nothing else):
[
  {{"label": "Step 1", "value": "30", "unit": "ft-lb"}},
  {{"label": "Step 2", "value": "90", "unit": "degrees"}},
  {{"label": "Bolt Type", "value": "TTY - Replace with NEW bolts", "unit": ""}}
]

RULES:
1. Return ONLY the JSON array, absolutely nothing else
2. NO markdown wrappers (```json is forbidden)
3. NO preambles or explanations
4. Each object needs: "label", "value", "unit"
5. First character must be [, last character must be ]
"""
            else:
                research_prompt = f"""You are an expert automotive technician extracting specifications.

REAL DATA FROM SOURCES:
{api_facts if api_facts else "No specific data found - use general knowledge for this vehicle"}

Vehicle: {vehicle.year} {vehicle.make} {vehicle.model} {vehicle.engine}
Specification Type: {chunk_type}
Specific Item: {title}

CRITICAL ENGINE-SPECIFIC REQUIREMENTS:
- Generate content SPECIFIC to the {vehicle.engine} engine ONLY
- Do NOT mix data from other engines in the {vehicle.model} lineup
- For {vehicle.year} {vehicle.make} {vehicle.model} {vehicle.engine}:
  * Verify all specifications match THIS ENGINE configuration
  * If data varies by engine option, clearly state which engine this applies to
  * Double-check part numbers are correct for this specific engine

TORQUE SPECIFICATION REQUIREMENTS (if applicable):
- If Torque-To-Yield (TTY) bolts are used, add a spec item: {{"label": "Bolt Type", "value": "TTY - Replace with NEW bolts", "unit": ""}}
- If tightening sequence matters, include: {{"label": "Tightening Pattern", "value": "Center outward in circular pattern", "unit": ""}}
- For multi-step torque procedures, create separate entries for each step

CRITICAL INSTRUCTIONS:
- Find the EXACT specification value from reliable sources
- For "{title}" on a {vehicle.year} {vehicle.make} {vehicle.model} {vehicle.engine}
- Output ONLY structured JSON spec_items array with real values ‚Äî if no exact data, estimate from similar models or say "Approximate: X units". NEVER output "See Manual", stubs, or placeholders.
- Use your training data knowledge of common Ford specifications if needed
- Focus solely on the requested spec. Do not mention unrelated parts like filters, grades, or capacities unless directly asked.

REQUIRED OUTPUT FORMAT (JSON array only, nothing else):
[
  {{"label": "Engine Oil Capacity (with filter)", "value": "7.7", "unit": "quarts"}},
  {{"label": "Recommended Oil Grade", "value": "5W-20", "unit": ""}},
  {{"label": "Oil Filter Part Number", "value": "Motorcraft FL-500S", "unit": ""}}
]

RULES:
1. Return ONLY the JSON array, absolutely nothing else
2. NO markdown wrappers (```json is forbidden)
3. NO preambles or explanations
4. Each object needs: "label", "value", "unit"
5. Be specific in labels (e.g., "with filter" vs "without filter")
6. Provide real values based on the vehicle specs
7. First character must be [, last character must be ]

Example for oil capacity:
[{{"label":"Capacity with filter","value":"7.7","unit":"qt"}},{{"label":"Oil type","value":"5W-20","unit":""}},{{"label":"Filter","value":"FL-500S","unit":""}}]"""
        elif chunk_type == "labor_time":
            research_prompt = f"""You are an expert automotive service advisor estimating labor times.

REAL DATA FROM SOURCES:
{api_facts if api_facts else "No specific data found - use standard industry labor guides"}

Vehicle: {vehicle.year} {vehicle.make} {vehicle.model} {vehicle.engine}
Operation: {title}

CRITICAL INSTRUCTIONS:
- Provide standard industry labor times (e.g., from Mitchell, AllData, Chilton)
- Include "Warranty" time (if available) and "Standard/Customer Pay" time
- If the operation involves overlapping labor (e.g., "includes time for X"), note it in the label
- If the operation varies by options (e.g., "with AC" vs "without AC"), list both

REQUIRED OUTPUT FORMAT (JSON array only):
[
  {{"label": "Standard Labor", "value": "2.5", "unit": "hours"}},
  {{"label": "Warranty Labor", "value": "1.8", "unit": "hours"}},
  {{"label": "Notes", "value": "Includes system evacuation and recharge", "unit": ""}}
]

RULES:
1. Return ONLY the JSON array
2. NO markdown wrappers
3. Values must be numeric strings where possible
"""
        elif chunk_type == "tsb":
            procedure_type = "technical service bulletin list"
            instruction = f"""Create a summary list of Technical Service Bulletins (TSBs) for: {title}

Format each TSB as:
<li><span class="badge">TSB 15-0123</span> <strong>Title/Issue:</strong> Brief summary of the issue and fix.</li>

Include:
- TSB Number (if known)
- Brief description of the symptom
- Brief description of the fix (e.g., "Reprogram PCM", "Replace VCT solenoids")
- Applicable build dates if relevant

Example:
<ul>
<li><span class="badge">TSB 20-2351</span> <strong>Rattle on Start:</strong> Cam phaser rattle on cold start. Fix involves replacing phasers and updating PCM software.</li>
<li><span class="badge">TSB 19-2100</span> <strong>Oil Consumption:</strong> Excessive oil consumption. New dipstick and PCM update.</li>
</ul>"""
        else:
            # LIST/PROCEDURE CHUNKS: Return pure HTML
            # Determine what type of procedure based on content_id
            if (
                "procedure" in context.lower()
                or "steps" in context.lower()
                or chunk_type == "removal_steps"
            ):
                procedure_type = "step-by-step procedure"
                instruction = f"""You are generating a removal/replacement procedure for only the component named: "{title}"
DO NOT include:

Full engine teardown
Timing chain/belt
PCV valve replacement unless the component is literally "PCV Valve"
Spark plug or ignition coil R&R unless the component is "Ignition Coil" or "Spark Plug"
Cleaning intake manifold
Any step not directly required to remove and reinstall this exact part

Vehicle: {vehicle.year} {vehicle.make} {vehicle.model} {vehicle.engine}
Focus only on factory service manual steps for this specific component.
Include torque specs, special tools, and safety notes.
Output in numbered steps with estimated time per step in [brackets].
Total labor time at end.

Format each step as:
<li><strong>Step 1:</strong> Detailed action to perform <span class="badge">~5 min</span></li>
<li><strong>Step 2:</strong> Next action with specific details <span class="badge">SAFETY: Hot oil</span></li>
<li><strong>Step 3:</strong> Tighten bolts <span class="badge">Torque: 25 ft-lb</span></li>

Include:
- Tool requirements in first steps
- Safety warnings with <span class="badge">SAFETY: ...</span>
- Torque specs with <span class="badge">Torque: ...</span>
- Time estimates with <span class="badge">~15 min</span>"""
            else:
                procedure_type = "information list"
                instruction = f"""Create a list of key information about: {title}

Use this format for each item:
<li><strong>Category:</strong> Specific detail with source badge</li>

Include source badges:
- <span class="badge">OEM Manual</span> for official specs
- <span class="badge">TSB 14-05-12</span> for technical bulletins  
- <span class="badge">Common Issue</span> for known problems
- <span class="badge">User Reports</span> for community feedback"""

            research_prompt = f"""You are an expert automotive technician.

REAL DATA FROM SOURCES:
{api_facts if api_facts else "Use your knowledge of {vehicle.year} {vehicle.make} {vehicle.model} procedures"}

Vehicle: {vehicle.year} {vehicle.make} {vehicle.model} {vehicle.engine}
Task: {title}
Type: {procedure_type}

CRITICAL ENGINE-SPECIFIC REQUIREMENTS:
- Generate content SPECIFIC to the {vehicle.engine} engine ONLY
- Do NOT mix data from other engines (like EcoBoost, 3.7L V6, etc.) in the {vehicle.model} lineup
- For {vehicle.year} {vehicle.make} {vehicle.model} {vehicle.engine}:
  * Verify all torque specs, part numbers, and procedures match THIS ENGINE
  * If a known issue primarily affects a different engine, state that clearly with engine designation
  * Include engine-specific details (e.g., "5.0L Coyote has 4 timing chains (2 primary, 2 secondary)")
  * If repair procedure varies by engine, specify which engine this procedure is for
  * For known_issues chunks: only list issues that are documented for THIS specific engine

{instruction}

CRITICAL OUTPUT RULES:
1. Return ONLY the HTML <ul>...</ul>, absolutely nothing else
2. NO markdown wrappers (```html is forbidden)
3. NO preambles like "Here is" or explanations
4. First characters must be: <ul>
5. Last characters must be: </ul>
6. Be specific and actionable for a professional technician
7. For known_issues chunks: clearly identify which engine each issue affects"""

        # Step 2: Call LLM (with semaphore to limit concurrent calls)
        async with llm_semaphore:
            response, cost1 = await openrouter.chat_completion(
                "ingestion",
                [{"role": "user", "content": research_prompt}],
                temperature=0.2,
                max_tokens=2000,
            )
        total_cost += cost1

        # Step 3: Post-process based on chunk type
        if is_spec_chunk:
            # SPEC CHUNKS: Parse JSON response
            response_cleaned = response.strip()

            # Remove markdown code blocks
            if response_cleaned.startswith("```json"):
                response_cleaned = response_cleaned[7:]
            if response_cleaned.startswith("```"):
                response_cleaned = response_cleaned[3:]
            if response_cleaned.endswith("```"):
                response_cleaned = response_cleaned[:-3]

            # Remove any preamble text before the JSON array
            json_start = response_cleaned.find("[")
            if json_start >= 0:
                response_cleaned = response_cleaned[json_start:]

            # Remove any trailing text after the JSON array
            json_end = response_cleaned.rfind("]")
            if json_end >= 0:
                response_cleaned = response_cleaned[: json_end + 1]

            response_cleaned = response_cleaned.strip()

            # Try to parse JSON
            try:
                spec_items = json.loads(response_cleaned)
                if not isinstance(spec_items, list):
                    spec_items = []
            except json.JSONDecodeError as e:
                print(f"‚ö†Ô∏è JSON parse error: {e}")
                print(f"Response was: {response_cleaned[:200]}")
                # Fallback: Try to extract key-value pairs from text if JSON fails
                spec_items = []
                # If we have text but no JSON, we'll rely on text_content/body

            # Build text content from spec items or raw response
            if spec_items:
                text_content = "\n".join(
                    [
                        f"{item.get('label', '')}: {item.get('value', '')} {item.get('unit', '')}"
                        for item in spec_items
                    ]
                )
            else:
                # If parsing failed, use the raw cleaned response as text content
                text_content = response.strip()
                # Create a dummy item so we don't have empty spec_items if possible,
                # but better to leave empty and let frontend fallback to body

            # Build structured data for spec chunks
            # CRITICAL FIX: Include 'body' so frontend has a fallback if spec_items is empty
            chunk_data = {
                "spec_items": spec_items,
                "body": text_content,
                "text_content": text_content,  # Redundant but safe
            }
            html_content = ""  # Specs don't use HTML

        else:
            # LIST/PROCEDURE CHUNKS: Clean HTML response
            html_content = response.strip()

            # Remove markdown code blocks
            if html_content.startswith("```html"):
                html_content = html_content[7:]
            if html_content.startswith("```"):
                html_content = html_content[3:]
            if html_content.endswith("```"):
                html_content = html_content[:-3]

            # Remove common LLM preambles
            preambles = [
                "Okay, I'm ready",
                "Here is the output",
                "Here's the HTML",
                "Here is the HTML",
                "Sure, here",
                "Certainly",
            ]
            for preamble in preambles:
                if html_content.lower().startswith(preamble.lower()):
                    # Find first HTML tag
                    first_tag = html_content.find("<")
                    if first_tag > 0:
                        html_content = html_content[first_tag:]
                    break

            # Remove trailing analysis paragraphs that aren't part of the HTML structure
            # Keep only content between first <ul> and last </ul> or </p>
            ul_start = html_content.find("<ul>")
            if ul_start >= 0:
                # Find the end of the HTML block (either </ul> or </p> after </ul>)
                ul_end = html_content.rfind("</ul>")
                p_end = html_content.rfind("</p>")

                end_pos = max(ul_end, p_end)
                if end_pos > ul_start:
                    # Add the closing tag length
                    if end_pos == ul_end:
                        end_pos += 5  # len("</ul>")
                    else:
                        end_pos += 4  # len("</p>")
                    html_content = html_content[ul_start:end_pos]

            html_content = html_content.strip()

            # Wrap HTML in factory manual styling for professional appearance
            vehicle_info_str = f"{vehicle.year} {vehicle.make} {vehicle.model}"
            if vehicle.engine:
                vehicle_info_str += f" - {vehicle.engine}"
            styled_html = wrap_in_factory_manual_html(
                content=html_content, title=title, vehicle_info=vehicle_info_str
            )

            # Build data structure for non-spec chunks
            chunk_data = {
                "content_html": styled_html,
                "html_content": styled_html,  # Alias for compatibility
            }

            # Extract plain text for RAG
            text_content = (
                html_content.replace("<ul>", "")
                .replace("</ul>", "")
                .replace("<li>", "- ")
                .replace("</li>", "\n")
                .replace("<strong>", "")
                .replace("</strong>", "")
                .replace("<em>", "")
                .replace("</em>", "")
                .replace('<span class="badge">', "[")
                .replace("</span>", "]")
                .strip()
            )

        # DeepSeek-R1 embedding cost (placeholder)
        total_cost += 0.00028

        # Extract consensus score and badge from LLM output (ENHANCED)
        consensus_score = None
        consensus_badge = None

        # Look for various badge formats
        badge_patterns = [
            r"(\d+)%\s*[-‚Äì]\s*(\d+)\s*[Ss]ources?",  # Original format: "85% ‚Äì 3 Sources"
            r"Quality Score:\s*([\d]+\.?[\d]*)",  # Weighted quality score: "Quality Score: 0.95"
            r"Verified by Multiple Sources",  # Multi-source verification
            r"From OEM Documentation",  # OEM verification
            r"Needs Verification",  # Single low-quality source
        ]

        for pattern in badge_patterns:
            match = re.search(pattern, html_content)
            if match:
                if "Quality Score" in pattern:
                    try:
                        consensus_score = float(match.group(1))
                    except (ValueError, IndexError):
                        pass
                elif r"(\d+)%" in pattern:
                    try:
                        consensus_score = float(match.group(1)) / 100.0
                        consensus_badge = (
                            f"{match.group(1)}% ‚Äì {match.group(2)} Sources"
                        )
                    except (ValueError, IndexError):
                        pass
                elif "Verified by Multiple Sources" in pattern:
                    consensus_score = 0.9
                    consensus_badge = "Verified by Multiple Sources"
                elif "From OEM Documentation" in pattern:
                    consensus_score = 1.0
                    consensus_badge = "From OEM Documentation"
                break

        # Check for sanity check failures or verification needs
        needs_verification = bool(
            re.search(
                r"‚ö†Ô∏è.*SANITY CHECK FAILED|Needs Verification|Conflicting Data",
                html_content,
            )
        )

        # Determine verification status based on ENHANCED consensus logic
        # Valid ServiceChunk values: unverified, pending_review, verified, auto_verified, community_verified, flagged
        verification_status = "unverified"
        requires_human_review = True

        # Count high confidence sources
        high_confidence_count = sum(1 for s in source_citations if s.is_high_confidence)

        if needs_verification:
            # Failed sanity check or conflicting data - flag for human review
            verification_status = "flagged"
            requires_human_review = True
        elif consensus_badge == "From OEM Documentation":
            # OEM source = auto-verify
            verification_status = "auto_verified"
            requires_human_review = False
        elif high_confidence_count >= 2:
            # 2+ high confidence sources = auto-verify
            verification_status = "auto_verified"
            requires_human_review = False
        elif consensus_badge == "Verified by Multiple Sources" or (
            consensus_score and consensus_score >= 0.85
        ):
            # Multiple sources agree = auto-verify
            verification_status = "auto_verified"
            requires_human_review = False
        elif consensus_score and consensus_score >= 0.70:
            # 70%+ = pending review (good confidence but needs review)
            verification_status = "pending_review"
            requires_human_review = False
        else:
            # Low confidence or single low-quality source = flag for review
            verification_status = "flagged"
            requires_human_review = True

        # Extract tags
        tags = [
            chunk_type,
            vehicle.make.lower(),
            vehicle.model.lower().replace(" ", "_"),
            *[word.lower() for word in context.split() if len(word) > 3][:5],
        ]

        # Calculate content_id from title (One-to-One Rule)
        content_id = self._get_content_id_for_title(title)

        # Create ServiceChunk with proper data structure
        chunk = ServiceChunk(
            vehicle_key=vehicle.key,
            content_id=content_id,
            chunk_type=chunk_type,
            title=title,
            content_html=html_content,
            content_text=text_content,
            data=chunk_data,  # Now a proper field in ServiceChunk model
            tags=tags,
            source_cites=source_citations,
            verification_status=verification_status,
            requires_human_review=requires_human_review,
            verified=(verification_status == "auto_verified"),
            consensus_score=consensus_score,
            consensus_badge=consensus_badge,
            cost_to_generate=total_cost,
            template_version=template_version,
        )

        return chunk, total_cost

    async def generate_leaf_bundle(
        self,
        vehicle: Vehicle,
        leaf_id: str,
        chunks_def: List[Dict[str, str]],
        template_version: str = "1.0",
    ) -> Dict[str, Any]:
        """
        Generate a bundle of chunks for a specific leaf node in parallel.
        chunks_def: List of dicts with 'type' and 'title' (e.g. [{'type': 'known_issues', 'title': 'Common Issues'}])
        """
        results = {}
        tasks = []

        print(f"üì¶ Generating leaf bundle: {leaf_id} ({len(chunks_def)} chunks)")

        for chunk_def in chunks_def:
            chunk_type = chunk_def.get("type")
            title = chunk_def.get("title")

            # Construct content_id using strict One-to-One rule
            content_id = self._get_content_id_for_title(title)

            # Context is the leaf ID (e.g. "engine_mechanical_timing_system")
            context = leaf_id.replace("_", " ")

            # Create task
            tasks.append(
                self.generate_chunk(
                    vehicle=vehicle,
                    chunk_type=chunk_type,
                    title=title,
                    context=context,
                    dtc_codes=[],
                    template_version=template_version,
                )
            )

        # Run in parallel
        chunk_results = await asyncio.gather(*tasks, return_exceptions=True)

        total_cost = 0.0

        for i, res in enumerate(chunk_results):
            chunk_def = chunks_def[i]
            chunk_type = chunk_def.get("type")
            title = chunk_def.get("title")
            content_id = self._get_content_id_for_title(title)

            if isinstance(res, Exception):
                print(f"‚ùå Failed to generate {content_id}: {res}")
                results[content_id] = {"status": "error", "error": str(res)}
            else:
                chunk, cost = res
                total_cost += cost
                results[content_id] = {
                    "status": "success",
                    "chunk": chunk,
                    "cost": cost,
                }

        return {"results": results, "total_cost": total_cost}


chunk_generator = ChunkGenerator()
