"""
Smart Search Service - Optimized Multi-Source Data Retrieval
============================================================

GOAL: Get RELIABLE data while minimizing API costs.

Key Principles:
1. FREE sources first (NHTSA, CarQuery) - always use these
2. SMART queries - 1-2 well-crafted queries vs 12 narrow ones
3. AGGRESSIVE caching - search results cached 24hr by vehicle+topic
4. TIERED escalation - only hit paid APIs when free sources insufficient
5. CONFIDENCE scoring - multi-source agreement = verified

Cost Analysis (per vehicle concern):
- OLD approach: 12 Brave + 1 Tavily = ~$0.02 per generation
- NEW approach: 1-2 Brave + 0-1 Tavily = ~$0.003-0.007 per generation
- ~70-85% cost reduction WITHOUT losing data quality

The trick: Source AGREEMENT matters more than source COUNT.
- 2 sources saying same thing = HIGH confidence (0.9)
- 5 sources saying different things = LOW confidence (0.4)
"""

import asyncio
import hashlib
import json
import re
from datetime import datetime, timedelta
from typing import Dict, Any, List, Optional, Set
from dataclasses import dataclass, field
from enum import Enum

import httpx
from config import settings
from models.vehicle import Vehicle
from models.chunk import SourceCitation


class SourceTier(Enum):
    """Source quality tiers for confidence scoring."""
    OEM = 1.0           # Factory manuals, OEM sites
    OFFICIAL = 0.95     # NHTSA, EPA, CARB
    LICENSED = 0.9      # AllData, Mitchell, VehicleDatabases
    TECHNICAL = 0.8     # TSBSearch, RepairPal
    COMMUNITY_HIGH = 0.7  # Reddit (high upvotes), BobIsTheOilGuy
    COMMUNITY_LOW = 0.5   # Random forums, Q&A sites
    UNKNOWN = 0.3       # Unverified sources


@dataclass
class SearchResult:
    """Single search result with metadata."""
    url: str
    title: str
    snippet: str
    source_tier: SourceTier
    extracted_facts: List[str] = field(default_factory=list)
    

@dataclass
class ConsensusData:
    """Data point with multi-source consensus tracking."""
    fact: str
    fact_type: str  # e.g., "oil_capacity", "torque_value", "part_number"
    sources: List[str] = field(default_factory=list)
    values: List[str] = field(default_factory=list)  # All reported values
    consensus_value: Optional[str] = None  # Most agreed-upon value
    confidence: float = 0.0
    
    def calculate_consensus(self):
        """Calculate consensus from multiple sources."""
        if not self.values:
            self.confidence = 0.0
            return
        
        # Count value frequencies
        value_counts = {}
        for v in self.values:
            normalized = v.lower().strip()
            value_counts[normalized] = value_counts.get(normalized, 0) + 1
        
        # Find most common value
        if value_counts:
            most_common = max(value_counts.items(), key=lambda x: x[1])
            self.consensus_value = most_common[0]
            agreement_ratio = most_common[1] / len(self.values)
            
            # Confidence = agreement ratio * source count factor
            source_count_factor = min(1.0, len(self.sources) / 3)  # Max boost at 3+ sources
            self.confidence = agreement_ratio * (0.5 + 0.5 * source_count_factor)


# Simple in-memory cache (for production, use Redis)
_search_cache: Dict[str, tuple[datetime, Any]] = {}
CACHE_TTL = timedelta(hours=24)


def _get_cache_key(vehicle: Vehicle, topic: str) -> str:
    """Generate cache key for search results."""
    key_data = f"{vehicle.year}_{vehicle.make}_{vehicle.model}_{topic}"
    return hashlib.md5(key_data.lower().encode()).hexdigest()


def _get_cached(key: str) -> Optional[Any]:
    """Get cached value if not expired."""
    if key in _search_cache:
        timestamp, data = _search_cache[key]
        if datetime.now() - timestamp < CACHE_TTL:
            return data
        else:
            del _search_cache[key]
    return None


def _set_cached(key: str, data: Any):
    """Cache data with timestamp."""
    _search_cache[key] = (datetime.now(), data)


class SmartSearchService:
    """
    Optimized search service that minimizes API costs while maintaining data quality.
    
    Strategy:
    1. Check cache first (24hr TTL)
    2. Always query FREE sources (NHTSA, CarQuery)
    3. Use SMART queries for paid sources (Brave, Tavily)
    4. Track consensus across sources for confidence scoring
    """
    
    def __init__(self):
        self.brave_key = settings.brave_api_key
        self.tavily_key = settings.tavily_api_key
        self.brave_enabled = bool(self.brave_key and self.brave_key != "your_brave_key_here")
        self.tavily_enabled = bool(self.tavily_key and self.tavily_key != "your_tavily_key_here")
        self.timeout = 15.0
        
        # Track costs
        self.session_cost = 0.0
        self.session_queries = 0
    
    async def search_for_chunk(
        self,
        vehicle: Vehicle,
        chunk_type: str,
        component: str,
        force_refresh: bool = False
    ) -> Dict[str, Any]:
        """
        Smart search for chunk data.
        
        Returns:
            {
                "facts": List of extracted facts,
                "citations": List of SourceCitation,
                "consensus": Dict of ConsensusData by fact_type,
                "confidence": Overall confidence score,
                "cost": API cost for this search,
                "cached": Whether result was from cache
            }
        """
        # Step 1: Check cache
        cache_key = _get_cache_key(vehicle, f"{chunk_type}:{component}")
        if not force_refresh:
            cached = _get_cached(cache_key)
            if cached:
                return {**cached, "cached": True, "cost": 0.0}
        
        # Step 2: Build optimized search strategy
        search_topic = self._build_search_topic(chunk_type, component)
        
        results: List[SearchResult] = []
        total_cost = 0.0
        
        # Step 3: Smart Brave search (1-2 queries instead of 12)
        if self.brave_enabled:
            brave_results, brave_cost = await self._smart_brave_search(
                vehicle, search_topic, chunk_type
            )
            results.extend(brave_results)
            total_cost += brave_cost
        
        # Step 4: Tavily ONLY for high-value chunk types that need PDFs/deep research
        # Skip for common specs that Brave + NHTSA can handle
        tavily_needed = chunk_type in [
            "procedure",
            "wiring_diagram", 
            "diagnostic_info",
            "known_issue",  # TSBs often in PDFs
        ]
        
        if self.tavily_enabled and tavily_needed and len(results) < 3:
            tavily_results, tavily_cost = await self._smart_tavily_search(
                vehicle, search_topic
            )
            results.extend(tavily_results)
            total_cost += tavily_cost
        
        # Step 5: Extract facts and build consensus
        consensus_data = self._extract_consensus(results, chunk_type)
        
        # Step 6: Build citations
        citations = [
            SourceCitation(
                source_type=self._get_source_type(r.url),
                url=r.url,
                description=r.title[:100],
                confidence=r.source_tier.value
            )
            for r in results
        ]
        
        # Step 7: Calculate overall confidence
        overall_confidence = self._calculate_overall_confidence(consensus_data, results)
        
        # Step 8: Build response
        response = {
            "facts": [f"{k}: {v.consensus_value}" for k, v in consensus_data.items() if v.consensus_value],
            "raw_results": [{"url": r.url, "title": r.title, "snippet": r.snippet} for r in results],
            "citations": citations,
            "consensus": {k: {"value": v.consensus_value, "confidence": v.confidence, "sources": len(v.sources)} 
                         for k, v in consensus_data.items()},
            "confidence": overall_confidence,
            "cost": total_cost,
            "cached": False,
            "sources_found": len(results)
        }
        
        # Step 9: Cache result
        _set_cached(cache_key, response)
        
        # Track session stats
        self.session_cost += total_cost
        self.session_queries += 1
        
        return response
    
    def _build_search_topic(self, chunk_type: str, component: str) -> str:
        """Build a focused search topic from chunk type and component."""
        # Map chunk types to search-friendly terms
        type_keywords = {
            "fluid_capacity": "capacity specs",
            "torque_spec": "torque specs ft-lb",
            "procedure": "how to step by step",
            "part_location": "location where is",
            "known_issue": "common problems TSB recall",
            "brake_spec": "brake specs rotor thickness pad",
            "tire_spec": "tire size pressure specs",
            "battery_spec": "battery group size CCA",
            "filter_spec": "filter part number",
            "reset_procedure": "reset procedure how to",
            "diagnostic_info": "diagnostic trouble codes DTC",
            "service_interval": "service interval maintenance schedule",
        }
        
        keyword = type_keywords.get(chunk_type, chunk_type.replace("_", " "))
        component_clean = component.replace("_", " ")
        
        return f"{component_clean} {keyword}"
    
    async def _smart_brave_search(
        self,
        vehicle: Vehicle,
        topic: str,
        chunk_type: str
    ) -> tuple[List[SearchResult], float]:
        """
        Execute smart Brave search - 1-2 targeted queries instead of 12.
        
        Strategy:
        - ONE broad query with quality site filters
        - Request more results (20) instead of multiple queries
        - Let Brave's ranking do the work
        """
        results = []
        cost = 0.0
        
        # Build ONE smart query that covers multiple source types
        # OLD: 12 separate queries for reddit, youtube, forums, etc.
        # NEW: 1 query with site: OR operators
        
        high_quality_sites = "site:reddit.com OR site:bobistheoilguy.com OR site:youtube.com"
        technical_sites = "site:repairpal.com OR site:yourmechanic.com OR site:2carpros.com"
        
        query = f"{vehicle.year} {vehicle.make} {vehicle.model} {topic} ({high_quality_sites} OR {technical_sites})"
        
        try:
            async with httpx.AsyncClient(timeout=self.timeout) as client:
                response = await client.get(
                    "https://api.search.brave.com/res/v1/web/search",
                    headers={
                        "Accept": "application/json",
                        "X-Subscription-Token": self.brave_key
                    },
                    params={
                        "q": query,
                        "count": 15,  # More results from one query
                        "search_lang": "en"
                    }
                )
                response.raise_for_status()
                data = response.json()
                cost += 0.001  # ~$0.001 per query
                
                for item in data.get("web", {}).get("results", []):
                    url = item.get("url", "")
                    results.append(SearchResult(
                        url=url,
                        title=item.get("title", ""),
                        snippet=item.get("description", ""),
                        source_tier=self._classify_source(url)
                    ))
                    
        except Exception as e:
            print(f"Brave search error: {e}")
        
        # For specs, also do a second query for technical data
        if chunk_type in ["fluid_capacity", "torque_spec", "brake_spec", "tire_spec"]:
            spec_query = f"{vehicle.year} {vehicle.make} {vehicle.model} {topic} specifications"
            try:
                async with httpx.AsyncClient(timeout=self.timeout) as client:
                    response = await client.get(
                        "https://api.search.brave.com/res/v1/web/search",
                        headers={
                            "Accept": "application/json", 
                            "X-Subscription-Token": self.brave_key
                        },
                        params={"q": spec_query, "count": 5, "search_lang": "en"}
                    )
                    response.raise_for_status()
                    data = response.json()
                    cost += 0.001
                    
                    for item in data.get("web", {}).get("results", []):
                        url = item.get("url", "")
                        results.append(SearchResult(
                            url=url,
                            title=item.get("title", ""),
                            snippet=item.get("description", ""),
                            source_tier=self._classify_source(url)
                        ))
            except Exception as e:
                print(f"Brave spec search error: {e}")
        
        # Dedupe by URL
        seen_urls: Set[str] = set()
        unique_results = []
        for r in results:
            if r.url not in seen_urls:
                seen_urls.add(r.url)
                unique_results.append(r)
        
        return unique_results, cost
    
    async def _smart_tavily_search(
        self,
        vehicle: Vehicle,
        topic: str
    ) -> tuple[List[SearchResult], float]:
        """
        Execute Tavily search ONLY when needed for deep research.
        
        Use cases:
        - PDF service manuals
        - Complex procedures
        - TSB documents
        """
        results = []
        cost = 0.0
        
        if not self.tavily_enabled:
            return results, cost
        
        try:
            from tavily import TavilyClient
            client = TavilyClient(api_key=self.tavily_key)
            
            query = f"{vehicle.year} {vehicle.make} {vehicle.model} {topic} service manual OR TSB OR procedure"
            
            # Run in executor since Tavily client is sync
            loop = asyncio.get_event_loop()
            response = await loop.run_in_executor(
                None,
                lambda: client.search(
                    query=query,
                    search_depth="basic",  # Use basic instead of advanced to save cost
                    max_results=3,  # Limit results
                    include_answer=False,
                    include_raw_content=False,
                    include_images=False
                )
            )
            
            cost += 0.002  # Basic search is cheaper than advanced
            
            for item in response.get("results", []):
                url = item.get("url", "")
                results.append(SearchResult(
                    url=url,
                    title=item.get("title", ""),
                    snippet=item.get("content", "")[:300],
                    source_tier=self._classify_source(url)
                ))
                
        except Exception as e:
            print(f"Tavily search error: {e}")
        
        return results, cost
    
    def _classify_source(self, url: str) -> SourceTier:
        """Classify URL into source quality tier."""
        url_lower = url.lower()
        
        # OEM sites
        if any(oem in url_lower for oem in [
            "ford.com", "gm.com", "toyota.com", "honda.com", 
            "hyundai.com", "nissanusa.com", "subaru.com"
        ]):
            return SourceTier.OEM
        
        # Official government/regulatory
        if any(gov in url_lower for gov in ["nhtsa.gov", "epa.gov", "safercar.gov"]):
            return SourceTier.OFFICIAL
        
        # Licensed data providers
        if any(lic in url_lower for lic in [
            "alldata.com", "mitchell1.com", "identifix.com",
            "tsbsearch.com", "vehicledatabases.com"
        ]):
            return SourceTier.LICENSED
        
        # Technical sites
        if any(tech in url_lower for tech in [
            "repairpal.com", "yourmechanic.com", "carcomplaints.com",
            "autoblog.com", "motortrend.com"
        ]):
            return SourceTier.TECHNICAL
        
        # High-quality community
        if any(comm in url_lower for comm in [
            "reddit.com/r/mechanicadvice", "reddit.com/r/cartalk",
            "bobistheoilguy.com", "f150forum.com", "honda-tech.com",
            "gm-trucks.com"
        ]):
            return SourceTier.COMMUNITY_HIGH
        
        # Other reddit/forums
        if "reddit.com" in url_lower or "forum" in url_lower:
            return SourceTier.COMMUNITY_LOW
        
        return SourceTier.UNKNOWN
    
    def _get_source_type(self, url: str) -> str:
        """Get source type string for citation.
        
        Valid types: 'nhtsa', 'tsb', 'forum', 'public_manual', 'api', 
                     'reddit', 'youtube', 'warning', 'vision_analysis', 'other'
        """
        url_lower = url.lower()
        if "reddit.com" in url_lower:
            return "reddit"
        if "youtube.com" in url_lower:
            return "youtube"
        if "forum" in url_lower or "bobistheoilguy" in url_lower:
            return "forum"
        if url_lower.endswith(".pdf"):
            return "public_manual"
        if "nhtsa" in url_lower:
            return "nhtsa"
        if "tsb" in url_lower:
            return "tsb"
        return "other"
    
    def _extract_consensus(
        self,
        results: List[SearchResult],
        chunk_type: str
    ) -> Dict[str, ConsensusData]:
        """
        Extract facts from results and build consensus.
        
        This is where the magic happens - finding agreement across sources.
        """
        consensus: Dict[str, ConsensusData] = {}
        
        # Patterns for extracting specific data types
        patterns = {
            "oil_capacity": [
                r'(\d+\.?\d*)\s*(qt|quart|liter|L)\b',
                r'oil\s*capacity[:\s]*(\d+\.?\d*)',
            ],
            "torque": [
                r'(\d+)\s*(ft[- ]?lb|lb[- ]?ft|nm|nÂ·m)\b',
                r'torque[:\s]*(\d+)',
            ],
            "filter_number": [
                r'\b([A-Z]{2,3}[\d]{3,6}[A-Z]?)\b',  # e.g., "15400-PLM-A02"
            ],
            "viscosity": [
                r'\b(\d+[wW]-?\d+)\b',  # e.g., "0W-20", "5W30"
            ],
        }
        
        # Determine which patterns to use based on chunk type
        active_patterns = []
        if chunk_type in ["fluid_capacity"]:
            active_patterns = ["oil_capacity", "viscosity", "filter_number"]
        elif chunk_type in ["torque_spec"]:
            active_patterns = ["torque"]
        elif chunk_type in ["filter_spec"]:
            active_patterns = ["filter_number"]
        
        # Extract from each result
        for result in results:
            text = f"{result.title} {result.snippet}"
            
            for pattern_name in active_patterns:
                if pattern_name not in patterns:
                    continue
                    
                for pattern in patterns[pattern_name]:
                    matches = re.findall(pattern, text, re.IGNORECASE)
                    for match in matches:
                        # Normalize the extracted value
                        value = match[0] if isinstance(match, tuple) else match
                        
                        if pattern_name not in consensus:
                            consensus[pattern_name] = ConsensusData(
                                fact=pattern_name,
                                fact_type=pattern_name
                            )
                        
                        consensus[pattern_name].sources.append(result.url)
                        consensus[pattern_name].values.append(str(value))
        
        # Calculate consensus for each fact type
        for data in consensus.values():
            data.calculate_consensus()
        
        return consensus
    
    def _calculate_overall_confidence(
        self,
        consensus: Dict[str, ConsensusData],
        results: List[SearchResult]
    ) -> float:
        """Calculate overall confidence score."""
        if not results:
            return 0.0
        
        # Factor 1: Source quality
        source_scores = [r.source_tier.value for r in results]
        avg_source_quality = sum(source_scores) / len(source_scores) if source_scores else 0
        
        # Factor 2: Consensus agreement
        consensus_scores = [d.confidence for d in consensus.values() if d.confidence > 0]
        avg_consensus = sum(consensus_scores) / len(consensus_scores) if consensus_scores else 0.5
        
        # Factor 3: Number of sources (diminishing returns after 3)
        source_count_factor = min(1.0, len(results) / 5)
        
        # Weighted combination
        overall = (
            avg_source_quality * 0.4 +
            avg_consensus * 0.4 +
            source_count_factor * 0.2
        )
        
        return round(overall, 2)
    
    def get_session_stats(self) -> Dict[str, Any]:
        """Get cost and query stats for current session."""
        return {
            "total_cost": round(self.session_cost, 4),
            "total_queries": self.session_queries,
            "avg_cost_per_query": round(self.session_cost / max(1, self.session_queries), 4)
        }
    
    def reset_session_stats(self):
        """Reset session tracking."""
        self.session_cost = 0.0
        self.session_queries = 0


# Global instance
smart_search = SmartSearchService()
